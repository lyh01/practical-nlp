{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hostname: 479ea4b9fba5, IP: 172.17.0.2\n\nNetwork interfaces: ['lo', 'sit0', 'eth0']\n[{17: [{'addr': '00:00:00:00:00:00', 'peer': '00:00:00:00:00:00'}], 2: [{'addr': '127.0.0.1', 'netmask': '255.0.0.0', 'peer': '127.0.0.1'}]}, {17: [{'addr': '00:00:00:00', 'broadcast': '00:00:00:00'}]}, {17: [{'addr': '02:42:ac:11:00:02', 'broadcast': 'ff:ff:ff:ff:ff:ff'}], 2: [{'addr': '172.17.0.2', 'netmask': '255.255.0.0', 'broadcast': '172.17.255.255'}]}]\n\nCurrent user: root, uid: 0, shell: /bin/bash \n\nCurrent directory: /home/hlaz001/practicalnlp/Ch5\n\nList current diretory:  ['06_EntityLinking-AzureTextAnalytics.ipynb', '02_NERTraining.ipynb', 'README.md', '04_NER_using_spaCy - CoNLL.ipynb', '05_BERT_CONLL_NER.ipynb', 'model', '08_Duckling.ipynb', '07_REWatson.ipynb', 'Data', '01_KPE.ipynb', '03_NERIssues.ipynb', 'spacyNER_data']\n\nPython vesion:  3.8.5 (default, Jul 28 2020, 12:59:40)  [GCC 9.3.0] \n\nPython path:  ['/home/hlaz001/practicalnlp/Ch5', '/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.3.684299474/pythonFiles/vscode_datascience_helpers/../.does-not-exist', '/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.3.684299474/pythonFiles', '/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.3.684299474/pythonFiles/lib/python', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/root/.local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages', '/root/.local/lib/python3.8/site-packages/IPython/extensions', '/root/.ipython'] \n\n"
     ]
    }
   ],
   "source": [
    "import os, pwd, sys, netifaces, socket\n",
    "\n",
    "print(f\"Hostname: {socket.gethostname()}, IP: {socket.gethostbyname(socket.gethostname())}\")\n",
    "print(f\"\\nNetwork interfaces: {netifaces.interfaces()}\")\n",
    "print([netifaces.ifaddresses(i) for i in netifaces.interfaces()])\n",
    "print(f\"\\nCurrent user: {pwd.getpwuid(os.geteuid()).pw_name}, uid: {pwd.getpwuid(os.geteuid()).pw_uid}, shell: {pwd.getpwuid(os.geteuid()).pw_shell} \")\n",
    "print(f\"\\nCurrent directory: {os.getcwd()}\")\n",
    "print(\"\\nList current diretory: \",[i for i in os.listdir()])\n",
    "print(\"\\nPython vesion: \",sys.version.replace(\"\\n\",\" \"), \"\\n\\nPython path: \",sys.path,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating an NER model with spaCy on the CoNLL dataset\n",
    "\n",
    "In this notebook, we will take a look at using spaCy commandline to train and evaluate a NER model. We will also compare it with the pretrained NER model in spacy. \n",
    "\n",
    "Note: we will create multiple folders during this experiment:\n",
    "spacyNER_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Converting data to json structures so it can be used by Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (14987 documents): spacyNER_data/train.json\u001b[0m\n",
      "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (3684 documents): spacyNER_data/test.json\u001b[0m\n",
      "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (3466 documents): spacyNER_data/valid.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Read the CONLL data from conll2003 folder, and store the formatted data into a folder spacyNER_data\n",
    "#!mkdir spacyNER_data\n",
    "#the above two lines create folders if they don't exist. If they do, the output shows a message that it\n",
    "#already exists and cannot be created again\n",
    "!python3.8 -m spacy convert \"Data/conll2003/en/train.txt\" spacyNER_data -c ner\n",
    "!python3.8 -m spacy convert \"Data/conll2003/en/test.txt\" spacyNER_data -c ner\n",
    "!python3.8 -m spacy convert \"Data/conll2003/en/valid.txt\" spacyNER_data -c ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For example, the data before and after running spacy's convert program looks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BEFORE : (Data/conll2003/en/train.txt)\n",
      "EU NNP B-NP B-ORG\n",
      "rejects VBZ B-VP O\n",
      "German JJ B-NP B-MISC\n",
      "call NN I-NP O\n",
      "to TO B-VP O\n",
      "boycott VB I-VP O\n",
      "British JJ B-NP B-MISC\n",
      "lamb NN I-NP O\n",
      ". . O O\n",
      "\\nAFTER : (Data/conll2003/en/train.json)\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\":1,\n",
      "    \"paragraphs\":[\n",
      "      {\n",
      "        \"sentences\":[\n",
      "          {\n",
      "            \"tokens\":[\n",
      "              {\n",
      "                \"orth\":\"EU\",\n",
      "                \"tag\":\"NNP\",\n",
      "                \"ner\":\"U-ORG\"\n",
      "              },\n",
      "              {\n",
      "                \"orth\":\"rejects\",\n",
      "                \"tag\":\"VBZ\",\n",
      "                \"ner\":\"O\"\n",
      "              },\n",
      "              {\n",
      "                \"orth\":\"German\",\n",
      "                \"tag\":\"JJ\",\n",
      "                \"ner\":\"U-MISC\"\n",
      "              },\n",
      "              {\n",
      "                \"orth\":\"call\",\n",
      "                \"tag\":\"NN\",\n",
      "                \"ner\":\"O\"\n",
      "              },\n",
      "              {\n",
      "                \"orth\":\"to\",\n",
      "                \"tag\":\"TO\",\n",
      "                \"ner\":\"O\"\n",
      "              },\n",
      "              {\n",
      "                \"orth\":\"boycott\",\n",
      "                \"tag\":\"VB\",\n",
      "                \"ner\":\"O\"\n",
      "              },\n",
      "              {\n",
      "                \"orth\":\"British\",\n",
      "                \"tag\":\"JJ\",\n",
      "                \"ner\":\"U-MISC\"\n",
      "              },\n",
      "              {\n",
      "                \"orth\":\"lamb\",\n",
      "                \"tag\":\"NN\",\n"
     ]
    }
   ],
   "source": [
    "!echo \"BEFORE : (Data/conll2003/en/train.txt)\"\n",
    "!head \"Data/conll2003/en/train.txt\" -n 11 | tail -n 9\n",
    "!echo \"\\nAFTER : (Data/conll2003/en/train.json)\"\n",
    "!head \"spacyNER_data/train.json\" -n 64 | tail -n 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NER model with Spacy (CLI)\n",
    "\n",
    "All the commandline options can be seen at: https://spacy.io/api/cli#train\n",
    "We are training using the train program in spacy, for English (en), and the results are stored in a folder \n",
    "called \"model\" (created while training). Our training file is in \"spacyNER_data/train.json\" and the validation file is at: \"spacyNER_data/valid.json\". \n",
    "\n",
    "-G stands for gpu option.\n",
    "-p stands for pipeline, and it should be followed by a comma separated set of options - in this case, a tagger and an NER are being trained simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[38;5;3m⚠ Output directory is not empty\u001b[0m\n",
      "This can lead to unintended side effects when saving the model. Please use an\n",
      "empty directory or a different path instead. If the specified output path\n",
      "doesn't exist, the directory will be created for you.\n",
      "Training pipeline: ['tagger', 'ner']\n",
      "Starting with blank model 'en'\n",
      "Counting training words (limit=0)\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/language.py:632: UserWarning: [W022] Training a new part-of-speech tagger using a model with no lemmatization rules or data. This means that the trained model may not be able to lemmatize correctly. If this is intentional or the language you're using doesn't have lemmatization data, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed.\n",
      "  proc.begin_training(\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/language.py:632: UserWarning: [W033] Training a new part-of-speech tagger using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  proc.begin_training(\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/language.py:632: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  proc.begin_training(\n",
      "\n",
      "Itn  Tag Loss    Tag %    NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n",
      "---  ---------  --------  ---------  ------  ------  ------  -------  -------\n",
      "  1  31195.450    94.156  16637.800  83.889  82.548  83.213  100.000     1615\n",
      "  2  16760.172    94.790   7593.302  86.230  85.577  85.903  100.000     3232\n",
      "  3  13676.140    95.112   5070.023  87.583  87.008  87.294  100.000     2848\n",
      "  4  11656.122    95.296   3918.050  88.118  87.614  87.865  100.000     2315\n",
      "  5  10457.547    95.337   3029.802  88.579  88.236  88.407  100.000     2592\n",
      "  6   9599.569    95.386   2639.399  88.782  88.438  88.610  100.000     2385\n",
      "  7   8889.604    95.452   2241.124  88.996  88.607  88.801  100.000     1447\n",
      "  8   8422.404    95.494   2020.014  89.159  88.724  88.941  100.000     3467\n",
      "  9   7903.826    95.488   1793.993  89.048  88.808  88.928  100.000     3901\n",
      " 10   7455.404    95.519   1745.534  89.214  88.808  89.011  100.000     2012\n",
      " 11   7186.674    95.554   1591.866  89.092  88.657  88.874  100.000      365\n",
      " 12   6688.416    95.564   1486.097  88.889  88.590  88.739  100.000     7108\n",
      " 13   6516.359    95.550   1354.268  88.827  88.573  88.700  100.000     6879\n",
      " 14   6370.103    95.566   1299.398  88.831  88.607  88.719  100.000     4536\n",
      " 15   6048.527    95.564   1145.796  88.722  88.573  88.647  100.000     3552\n",
      " 16   5699.702    95.558   1086.962  88.806  88.657  88.732  100.000     2898\n",
      " 17   5577.964    95.572   1092.005  88.851  88.522  88.687  100.000     2457\n",
      " 18   5423.305    95.585   1029.120  88.908  88.623  88.765  100.000     2072\n",
      " 19   5154.305    95.585   1001.599  88.782  88.573  88.677  100.000     3120\n",
      " 20   5130.158    95.589    968.972  88.788  88.623  88.705  100.000     3715\n",
      " 21   4924.836    95.609    955.190  88.872  88.708  88.790  100.000     3718\n",
      " 22   4779.749    95.591    961.792  89.031  88.792  88.911  100.000     1888\n",
      " 23   4599.615    95.599    842.900  88.784  88.590  88.687  100.000     2535\n",
      " 24   4473.259    95.578    905.566  88.688  88.539  88.614  100.000     2071\n",
      " 25   4383.460    95.547    941.688  88.580  88.371  88.475  100.000     1582\n",
      " 26   4287.336    95.593    794.103  88.628  88.405  88.516  100.000     1438\n",
      " 27   4179.561    95.605    788.660  88.499  88.320  88.410  100.000     1872\n",
      " 28   4043.866    95.601    786.112  88.254  88.135  88.195  100.000     1688\n",
      " 29   3897.405    95.632    726.130  88.501  88.337  88.419  100.000     1469\n",
      " 30   3932.820    95.636    647.821  88.423  88.304  88.363  100.000     1319\n",
      "\u001b[38;5;2m✔ Saved model to output directory\u001b[0m\n",
      "model/model-final\n",
      "\u001b[2K\u001b[38;5;2m✔ Created best model\u001b[0m\n",
      "model/model-best\n"
     ]
    }
   ],
   "source": [
    "!python3.8 -m spacy train en model spacyNER_data/train.json spacyNER_data/valid.json -G -p tagger,ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the performance improves with each iteration!\n",
    "## Evaluating the model with test data set (`spacyNER_data/test.json`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Trained model (`model/model-best`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\n================================== Results ==================================\u001b[0m\n\nTime      40.14 s\nWords     46666  \nWords/s   1163   \nTOK       100.00 \nPOS       95.17  \nUAS       0.00   \nLAS       0.00   \nNER P     81.39  \nNER R     81.71  \nNER F     81.55  \nTextcat   0.00   \n\n/usr/local/lib/python3.8/dist-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n  warnings.warn(Warnings.W006)\n\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\nresult\n"
     ]
    }
   ],
   "source": [
    "#create a folder to store the output and visualizations. \n",
    "!mkdir result\n",
    "!python3.8 -m spacy evaluate model/model-best spacyNER_data/test.json -dp result\n",
    "# !python -m spacy evaluate model/model-final data/test.txt.json -dp result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a Visualization of the entity tagged test data can be seen in result/entities.html folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On spacy's Pretrained NER model (`en`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Traceback (most recent call last):\n  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.8/dist-packages/spacy/__main__.py\", line 33, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/usr/local/lib/python3.8/dist-packages/plac_core.py\", line 367, in call\n    cmd, result = parser.consume(arglist)\n  File \"/usr/local/lib/python3.8/dist-packages/plac_core.py\", line 232, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/spacy/cli/evaluate.py\", line 49, in evaluate\n    nlp = util.load_model(model)\n  File \"/usr/local/lib/python3.8/dist-packages/spacy/util.py\", line 175, in load_model\n    raise IOError(Errors.E050.format(name=name))\nOSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n"
     ]
    }
   ],
   "source": [
    "#!mkdir pretrained_result\n",
    "!python3.8 -m spacy evaluate en spacyNER_data/test.json -dp pretrained_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a Visualization of the entity tagged test data can be seen in pretrained_result/entities.html folder. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}